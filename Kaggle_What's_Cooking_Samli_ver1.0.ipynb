{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "84677029",
   "metadata": {},
   "source": [
    "# My own model for Kaggle: What's cooking (SamLi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "82c4e859",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe86d433",
   "metadata": {},
   "source": [
    "## 1: Read the data & try to find some features without using CountVectorizer firstly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5260560b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cooking = pd.read_json(\"C:/Users/lijin/Desktop/ML-text-main/data/train.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7dc31d6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>cuisine</th>\n",
       "      <th>ingredients</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10259</td>\n",
       "      <td>greek</td>\n",
       "      <td>[romaine lettuce, black olives, grape tomatoes...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>25693</td>\n",
       "      <td>southern_us</td>\n",
       "      <td>[plain flour, ground pepper, salt, tomatoes, g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20130</td>\n",
       "      <td>filipino</td>\n",
       "      <td>[eggs, pepper, salt, mayonaise, cooking oil, g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>22213</td>\n",
       "      <td>indian</td>\n",
       "      <td>[water, vegetable oil, wheat, salt]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13162</td>\n",
       "      <td>indian</td>\n",
       "      <td>[black pepper, shallots, cornflour, cayenne pe...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id      cuisine                                        ingredients\n",
       "0  10259        greek  [romaine lettuce, black olives, grape tomatoes...\n",
       "1  25693  southern_us  [plain flour, ground pepper, salt, tomatoes, g...\n",
       "2  20130     filipino  [eggs, pepper, salt, mayonaise, cooking oil, g...\n",
       "3  22213       indian                [water, vegetable oil, wheat, salt]\n",
       "4  13162       indian  [black pepper, shallots, cornflour, cayenne pe..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cooking.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f22cebc6",
   "metadata": {},
   "source": [
    "- Literally, we can find there are 3 types of column\n",
    "  - for prediction, it's easily to find \"id\" column is useless\n",
    "  - cuisine is the target output type\n",
    "  - ingredients column is what we need\n",
    "    - However, the question now is how to use the ingredients column to create some useful features?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "49698e71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         9\n",
       "1        11\n",
       "2        12\n",
       "3         4\n",
       "4        20\n",
       "         ..\n",
       "39769    12\n",
       "39770     7\n",
       "39771    12\n",
       "39772    21\n",
       "39773    12\n",
       "Name: ingredients, Length: 39774, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#what about the length of the ingredients column\n",
    "cooking.ingredients.apply(len) #即观察有多少种原材料"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "829c94a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        12.000000\n",
       "1        10.090909\n",
       "2        10.333333\n",
       "3         6.750000\n",
       "4        10.100000\n",
       "           ...    \n",
       "39769    12.166667\n",
       "39770    17.000000\n",
       "39771     8.250000\n",
       "39772    13.142857\n",
       "39773    12.000000\n",
       "Name: ingredients, Length: 39774, dtype: float64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#what about the ingredients_length?\n",
    "cooking.ingredients.apply(lambda x: np.mean([len(item) for item in x])) #这里所求的是平均原材料所包含的英文字母个数的长度（包括空格）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "98d72fc1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        108\n",
       "1        111\n",
       "2        124\n",
       "3         27\n",
       "4        202\n",
       "        ... \n",
       "39769    146\n",
       "39770    119\n",
       "39771     99\n",
       "39772    276\n",
       "39773    144\n",
       "Name: ingredients, Length: 39774, dtype: int32"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#what about the ingredients_length?\n",
    "cooking.ingredients.apply(lambda x: np.sum([len(item) for item in x])) #这里所求的是原材料所包含的英文字母个数的总长度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "04580172",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "#当然，为了用CountVectorizer, 我们还应该将list类型的ingredients列中的值，转化为string类型\n",
    "print(type(cooking.ingredients[1]))\n",
    "print(type(cooking.ingredients.astype(str)[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99ba3f10",
   "metadata": {},
   "source": [
    "- 我们创建4列以包含我们上述所需要的四个新列\n",
    "  - 这里使用定义函数的方法（为了使读入test_data时不再写四遍定义参数）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "068e24ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_features(df):\n",
    "    df['num_ingredients'] = df.ingredients.apply(len) #长度\n",
    "    df['ingredients_length_mean'] = df.ingredients.apply(lambda x: np.mean([len(item) for item in x]))\n",
    "    df['ingredients_length_sum'] = df.ingredients.apply(lambda x: np.sum([len(item) for item in x])) \n",
    "    df['ingredients_str'] = df.ingredients.astype(str) #preprcessing of CountVectorizer()\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "adbd613e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>cuisine</th>\n",
       "      <th>ingredients</th>\n",
       "      <th>num_ingredients</th>\n",
       "      <th>ingredients_length_mean</th>\n",
       "      <th>ingredients_length_sum</th>\n",
       "      <th>ingredients_str</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10259</td>\n",
       "      <td>greek</td>\n",
       "      <td>[romaine lettuce, black olives, grape tomatoes...</td>\n",
       "      <td>9</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>108</td>\n",
       "      <td>['romaine lettuce', 'black olives', 'grape tom...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>25693</td>\n",
       "      <td>southern_us</td>\n",
       "      <td>[plain flour, ground pepper, salt, tomatoes, g...</td>\n",
       "      <td>11</td>\n",
       "      <td>10.090909</td>\n",
       "      <td>111</td>\n",
       "      <td>['plain flour', 'ground pepper', 'salt', 'toma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20130</td>\n",
       "      <td>filipino</td>\n",
       "      <td>[eggs, pepper, salt, mayonaise, cooking oil, g...</td>\n",
       "      <td>12</td>\n",
       "      <td>10.333333</td>\n",
       "      <td>124</td>\n",
       "      <td>['eggs', 'pepper', 'salt', 'mayonaise', 'cooki...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>22213</td>\n",
       "      <td>indian</td>\n",
       "      <td>[water, vegetable oil, wheat, salt]</td>\n",
       "      <td>4</td>\n",
       "      <td>6.750000</td>\n",
       "      <td>27</td>\n",
       "      <td>['water', 'vegetable oil', 'wheat', 'salt']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13162</td>\n",
       "      <td>indian</td>\n",
       "      <td>[black pepper, shallots, cornflour, cayenne pe...</td>\n",
       "      <td>20</td>\n",
       "      <td>10.100000</td>\n",
       "      <td>202</td>\n",
       "      <td>['black pepper', 'shallots', 'cornflour', 'cay...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id      cuisine                                        ingredients  \\\n",
       "0  10259        greek  [romaine lettuce, black olives, grape tomatoes...   \n",
       "1  25693  southern_us  [plain flour, ground pepper, salt, tomatoes, g...   \n",
       "2  20130     filipino  [eggs, pepper, salt, mayonaise, cooking oil, g...   \n",
       "3  22213       indian                [water, vegetable oil, wheat, salt]   \n",
       "4  13162       indian  [black pepper, shallots, cornflour, cayenne pe...   \n",
       "\n",
       "   num_ingredients  ingredients_length_mean  ingredients_length_sum  \\\n",
       "0                9                12.000000                     108   \n",
       "1               11                10.090909                     111   \n",
       "2               12                10.333333                     124   \n",
       "3                4                 6.750000                      27   \n",
       "4               20                10.100000                     202   \n",
       "\n",
       "                                     ingredients_str  \n",
       "0  ['romaine lettuce', 'black olives', 'grape tom...  \n",
       "1  ['plain flour', 'ground pepper', 'salt', 'toma...  \n",
       "2  ['eggs', 'pepper', 'salt', 'mayonaise', 'cooki...  \n",
       "3        ['water', 'vegetable oil', 'wheat', 'salt']  \n",
       "4  ['black pepper', 'shallots', 'cornflour', 'cay...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#在读入文件的过程中，直接把四列创建完成\n",
    "train = make_features(pd.read_json(\"C:/Users/lijin/Desktop/ML-text-main/data/train.json\"))\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "937b94da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0              greek\n",
       "1        southern_us\n",
       "2           filipino\n",
       "3             indian\n",
       "4             indian\n",
       "            ...     \n",
       "39769          irish\n",
       "39770        italian\n",
       "39771          irish\n",
       "39772        chinese\n",
       "39773        mexican\n",
       "Name: cuisine, Length: 39774, dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = train.cuisine\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ba206bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression #为了提高准确率，这里直接上svm分类器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "895365a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "vect = CountVectorizer(token_pattern=r\"'([a-z ]+)'\") #token_pattern=... 表明CountVectorizer如何提取字符，用正则形式表达出来。\n",
    "lg = LogisticRegression()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05fa6a06",
   "metadata": {},
   "source": [
    "- 这里表明当出现如“romaine lettuce”类似这样的ingredients type时，提取“romaine lettuce”为一整体token，而非\"romaine\"一个,\"lettuce\"一个"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "084bb86e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#读入待预测数据\n",
    "new = make_features(pd.read_json(\"C:/Users/lijin/Desktop/ML-text-main/data/test.json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8961a864",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>ingredients</th>\n",
       "      <th>num_ingredients</th>\n",
       "      <th>ingredients_length_mean</th>\n",
       "      <th>ingredients_length_sum</th>\n",
       "      <th>ingredients_str</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>18009</td>\n",
       "      <td>[baking powder, eggs, all-purpose flour, raisi...</td>\n",
       "      <td>6</td>\n",
       "      <td>9.333333</td>\n",
       "      <td>56</td>\n",
       "      <td>['baking powder', 'eggs', 'all-purpose flour',...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>28583</td>\n",
       "      <td>[sugar, egg yolks, corn starch, cream of tarta...</td>\n",
       "      <td>11</td>\n",
       "      <td>10.272727</td>\n",
       "      <td>113</td>\n",
       "      <td>['sugar', 'egg yolks', 'corn starch', 'cream o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>41580</td>\n",
       "      <td>[sausage links, fennel bulb, fronds, olive oil...</td>\n",
       "      <td>6</td>\n",
       "      <td>9.666667</td>\n",
       "      <td>58</td>\n",
       "      <td>['sausage links', 'fennel bulb', 'fronds', 'ol...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>29752</td>\n",
       "      <td>[meat cuts, file powder, smoked sausage, okra,...</td>\n",
       "      <td>21</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>252</td>\n",
       "      <td>['meat cuts', 'file powder', 'smoked sausage',...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>35687</td>\n",
       "      <td>[ground black pepper, salt, sausage casings, l...</td>\n",
       "      <td>8</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>104</td>\n",
       "      <td>['ground black pepper', 'salt', 'sausage casin...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id                                        ingredients  num_ingredients  \\\n",
       "0  18009  [baking powder, eggs, all-purpose flour, raisi...                6   \n",
       "1  28583  [sugar, egg yolks, corn starch, cream of tarta...               11   \n",
       "2  41580  [sausage links, fennel bulb, fronds, olive oil...                6   \n",
       "3  29752  [meat cuts, file powder, smoked sausage, okra,...               21   \n",
       "4  35687  [ground black pepper, salt, sausage casings, l...                8   \n",
       "\n",
       "   ingredients_length_mean  ingredients_length_sum  \\\n",
       "0                 9.333333                      56   \n",
       "1                10.272727                     113   \n",
       "2                 9.666667                      58   \n",
       "3                12.000000                     252   \n",
       "4                13.000000                     104   \n",
       "\n",
       "                                     ingredients_str  \n",
       "0  ['baking powder', 'eggs', 'all-purpose flour',...  \n",
       "1  ['sugar', 'egg yolks', 'corn starch', 'cream o...  \n",
       "2  ['sausage links', 'fennel bulb', 'fronds', 'ol...  \n",
       "3  ['meat cuts', 'file powder', 'smoked sausage',...  \n",
       "4  ['ground black pepper', 'salt', 'sausage casin...  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e90cc505",
   "metadata": {},
   "source": [
    "## 2: Create a pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3fd25291",
   "metadata": {},
   "outputs": [],
   "source": [
    "#这步后来我发现有点多余了，之前定义过一遍y了orz，写的不好\n",
    "X = train.ingredients_str\n",
    "y = train.cuisine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c182f15f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7c3d131d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = make_pipeline(vect, lg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "59ece5bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('countvectorizer', CountVectorizer(token_pattern=\"'([a-z ]+)'\")),\n",
       " ('logisticregression', LogisticRegression())]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe.steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d33e46aa",
   "metadata": {},
   "source": [
    "### 先用 cross_validation 测试一下"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e801aaa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "D:\\anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "D:\\anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "D:\\anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "D:\\anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7743250623833745"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "cross_val_score(pipe, X, y, cv=5, scoring='accuracy').mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "366c21ce",
   "metadata": {},
   "source": [
    "- lg表现还是不错的，牛！"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f732635",
   "metadata": {},
   "source": [
    "## 3: 把我们第一步找的那四个变量加到pipeline里面"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4ee72d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import make_column_transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bd3812b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorize 1 column, passthrough 3 columns, and drop the remaining columns\n",
    "ct = make_column_transformer(\n",
    "    (vect, 'ingredients_str'), #这列要使用CountVectorizer()\n",
    "    ('passthrough', ['num_ingredients','ingredients_length_mean','ingredients_length_sum']), #这些列直接保留就好\n",
    "    remainder = 'drop' #其它列删除\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c0b563d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(39774, 6253)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create the FEATURE MATRIX from the DataFrame\n",
    "X_dtm_manual = ct.fit_transform(train)\n",
    "X_dtm_manual.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03efa8c4",
   "metadata": {},
   "source": [
    "### 在此基础上，尝试cross_validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f678d9f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = make_pipeline(ct, lg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f9660a05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('columntransformer',\n",
       "  ColumnTransformer(transformers=[('countvectorizer',\n",
       "                                   CountVectorizer(token_pattern=\"'([a-z ]+)'\"),\n",
       "                                   'ingredients_str'),\n",
       "                                  ('passthrough', 'passthrough',\n",
       "                                   ['num_ingredients', 'ingredients_length_mean',\n",
       "                                    'ingredients_length_sum'])])),\n",
       " ('logisticregression', LogisticRegression())]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe.steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ab3e5f0",
   "metadata": {},
   "source": [
    "- 纪念一下，我nm第一次写成pipe = (ct,svc) 半天愣是没发现哪里有错orz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fafc9965",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4001614310569875"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\") #csdn说上述警告可以直接忽略，我也就直接忽略了，6\n",
    "cross_val_score(pipe, train, y, cv=5, scoring='accuracy').mean() #注意，这里是train，而非X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcd547d2",
   "metadata": {},
   "source": [
    "- 啊，发生甚么事了，准确度骤然下降"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "31417e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#尝试保留部分特征\n",
    "ct = make_column_transformer(\n",
    "    (vect, 'ingredients_str'), #这列要使用CountVectorizer()\n",
    "    ('passthrough', ['num_ingredients','ingredients_length_mean']), #这些列直接保留就好\n",
    "    remainder = 'drop' #其它列删除\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f60cd955",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(39774, 6252)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_dtm_manual = ct.fit_transform(train)\n",
    "X_dtm_manual.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "70e72be9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('columntransformer',\n",
       "  ColumnTransformer(transformers=[('countvectorizer',\n",
       "                                   CountVectorizer(token_pattern=\"'([a-z ]+)'\"),\n",
       "                                   'ingredients_str'),\n",
       "                                  ('passthrough', 'passthrough',\n",
       "                                   ['num_ingredients',\n",
       "                                    'ingredients_length_mean'])])),\n",
       " ('logisticregression', LogisticRegression())]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe = make_pipeline(ct, lg)\n",
    "pipe.steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6f7e0a12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6943732590617293"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "cross_val_score(pipe, train, y, cv=5, scoring='accuracy').mean() #注意，这里是train，而非X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87b96592",
   "metadata": {},
   "source": [
    "- 好些了，那我们就把'ingredients_length_sum'这个features删了，不要了"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ff7954a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#再尝试保留部分特征\n",
    "ct = make_column_transformer(\n",
    "    (vect, 'ingredients_str'), #这列要使用CountVectorizer()\n",
    "    ('passthrough', ['num_ingredients']), #这些列直接保留就好\n",
    "    remainder = 'drop' #其它列删除\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d6d1d5a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('columntransformer',\n",
       "  ColumnTransformer(transformers=[('countvectorizer',\n",
       "                                   CountVectorizer(token_pattern=\"'([a-z ]+)'\"),\n",
       "                                   'ingredients_str'),\n",
       "                                  ('passthrough', 'passthrough',\n",
       "                                   ['num_ingredients'])])),\n",
       " ('logisticregression', LogisticRegression())]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe = make_pipeline(ct, lg)\n",
    "pipe.steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "297584e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(39774, 6251)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_dtm_manual = ct.fit_transform(train)\n",
    "X_dtm_manual.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e59855c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7348521155664555"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "cross_val_score(pipe, train, y, cv=5, scoring='accuracy').mean() #注意，这里是train，而非X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3536f73",
   "metadata": {},
   "source": [
    "- 我放弃抵抗了，第一步找的3个其它特征全是“飞舞”，直接就用vect + lg就行了，也许在别的分类器下这3个特征还是有可能有用的。我估计美赛当时就是这么寄的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "120ef81b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorize 1 column, passthrough 3 columns, and drop the remaining columns\n",
    "ct = make_column_transformer(\n",
    "    (vect, 'ingredients_str'),\n",
    "    remainder = 'drop'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "73aa2e71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(39774, 6250)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_dtm_manual = ct.fit_transform(train)\n",
    "X_dtm_manual.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b9067c71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('columntransformer',\n",
       "  ColumnTransformer(transformers=[('countvectorizer',\n",
       "                                   CountVectorizer(token_pattern=\"'([a-z ]+)'\"),\n",
       "                                   'ingredients_str')])),\n",
       " ('logisticregression', LogisticRegression())]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe = make_pipeline(ct, lg)\n",
    "pipe.steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "daa8eb8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7743250623833745"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "cross_val_score(pipe, train, y, cv=5, scoring='accuracy').mean() #注意，这里是train，而非X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05bb5992",
   "metadata": {},
   "source": [
    "### GridSearchCV to find the best params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a5c1abb4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('columntransformer',\n",
       "  ColumnTransformer(transformers=[('countvectorizer',\n",
       "                                   CountVectorizer(token_pattern=\"'([a-z ]+)'\"),\n",
       "                                   'ingredients_str')])),\n",
       " ('logisticregression', LogisticRegression())]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe.steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d560f6bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "fa0ca439",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'columntransformer__countvectorizer__token_pattern': ['\\\\b\\\\w\\\\w+\\\\b',\n",
       "  \"'([a-z ]+)'\"],\n",
       " 'logisticregression__solver': ['liblinear', 'newton-cg', 'lbfgs', 'sag'],\n",
       " 'logisticregression__C': [0.5, 1]}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_grid = {}\n",
    "param_grid['columntransformer__countvectorizer__token_pattern'] = [r'\\b\\w\\w+\\b',r\"'([a-z ]+)'\"] \n",
    "param_grid['logisticregression__solver'] = ['liblinear','newton-cg','lbfgs','sag']\n",
    "param_grid['logisticregression__C'] = [0.5,1]\n",
    "param_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "12a1f232",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = GridSearchCV(pipe, param_grid, cv=5, scoring='accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "d8b5758b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 10min 17s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5,\n",
       "             estimator=Pipeline(steps=[('columntransformer',\n",
       "                                        ColumnTransformer(transformers=[('countvectorizer',\n",
       "                                                                         CountVectorizer(token_pattern=\"'([a-z \"\n",
       "                                                                                                       \"]+)'\"),\n",
       "                                                                         'ingredients_str')])),\n",
       "                                       ('logisticregression',\n",
       "                                        LogisticRegression())]),\n",
       "             param_grid={'columntransformer__countvectorizer__token_pattern': ['\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                                                               \"'([a-z \"\n",
       "                                                                               \"]+)'\"],\n",
       "                         'logisticregression__C': [0.5, 1],\n",
       "                         'logisticregression__solver': ['liblinear',\n",
       "                                                        'newton-cg', 'lbfgs',\n",
       "                                                        'sag']},\n",
       "             scoring='accuracy')"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time grid.fit(train, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "f6ed07bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7859909722892805\n",
      "{'columntransformer__countvectorizer__token_pattern': '\\\\b\\\\w\\\\w+\\\\b', 'logisticregression__C': 1, 'logisticregression__solver': 'liblinear'}\n"
     ]
    }
   ],
   "source": [
    "print(grid.best_score_)\n",
    "print(grid.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b57305b6",
   "metadata": {},
   "source": [
    "- 出乎意料的结果"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c3f401e",
   "metadata": {},
   "source": [
    "### RandomizedSearchCV to find the best params\n",
    "  - 因为C可以为大于0的浮点数，所以尝试使用RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "02717259",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "1cc46773",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'columntransformer__countvectorizer__token_pattern': ['\\\\b\\\\w\\\\w+\\\\b',\n",
       "  \"'([a-z ]+)'\"],\n",
       " 'columntransformer__countvectorizer__min_df': [1, 2, 3],\n",
       " 'logisticregression__C': <scipy.stats._distn_infrastructure.rv_continuous_frozen at 0x2c4c3cd3730>}"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import scipy as sp\n",
    "param_grid = {}\n",
    "#这里就不调solver参数了，多加了个CountVectorizer()中的参数，min_df\n",
    "param_grid['columntransformer__countvectorizer__token_pattern'] = [r\"\\b\\w\\w+\\b\", r\"'([a-z ]+)'\"]\n",
    "param_grid['columntransformer__countvectorizer__min_df'] = [1, 2, 3]\n",
    "param_grid['logisticregression__C'] = sp.stats.uniform(scale=1)\n",
    "param_grid"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7d9e134",
   "metadata": {},
   "source": [
    "- scipy.stats.uniform(scale=1) 是 Scipy 中用于表示均匀分布的对象，它表示在指定的区间内所有数值出现的概率相等的概率分布。该函数的参数 scale 表示分布的区间长度，即分布的上限和下限之差。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "0ab88118",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "ad6426f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "rand = RandomizedSearchCV(pipe, param_grid, cv=5, scoring='accuracy', n_iter=5, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "dd894d30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('columntransformer',\n",
       "  ColumnTransformer(transformers=[('countvectorizer',\n",
       "                                   CountVectorizer(token_pattern=\"'([a-z ]+)'\"),\n",
       "                                   'ingredients_str')])),\n",
       " ('logisticregression', LogisticRegression())]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe.steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "2917b50b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2min 42s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=5,\n",
       "                   estimator=Pipeline(steps=[('columntransformer',\n",
       "                                              ColumnTransformer(transformers=[('countvectorizer',\n",
       "                                                                               CountVectorizer(token_pattern=\"'([a-z \"\n",
       "                                                                                                             \"]+)'\"),\n",
       "                                                                               'ingredients_str')])),\n",
       "                                             ('logisticregression',\n",
       "                                              LogisticRegression())]),\n",
       "                   n_iter=5,\n",
       "                   param_distributions={'columntransformer__countvectorizer__min_df': [1,\n",
       "                                                                                       2,\n",
       "                                                                                       3],\n",
       "                                        'columntransformer__countvectorizer__token_pattern': ['\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                                                                              \"'([a-z \"\n",
       "                                                                                              \"]+)'\"],\n",
       "                                        'logisticregression__C': <scipy.stats._distn_infrastructure.rv_continuous_frozen object at 0x000002C4C3CD3730>},\n",
       "                   random_state=1, scoring='accuracy')"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time rand.fit(train, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "d1427c75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>params</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.772867</td>\n",
       "      <td>0.004583</td>\n",
       "      <td>{'columntransformer__countvectorizer__min_df':...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.773370</td>\n",
       "      <td>0.004474</td>\n",
       "      <td>{'columntransformer__countvectorizer__min_df':...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.783125</td>\n",
       "      <td>0.004872</td>\n",
       "      <td>{'columntransformer__countvectorizer__min_df':...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.782722</td>\n",
       "      <td>0.004799</td>\n",
       "      <td>{'columntransformer__countvectorizer__min_df':...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.782974</td>\n",
       "      <td>0.004323</td>\n",
       "      <td>{'columntransformer__countvectorizer__min_df':...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_test_score  std_test_score  \\\n",
       "0         0.772867        0.004583   \n",
       "1         0.773370        0.004474   \n",
       "2         0.783125        0.004872   \n",
       "3         0.782722        0.004799   \n",
       "4         0.782974        0.004323   \n",
       "\n",
       "                                              params  \n",
       "0  {'columntransformer__countvectorizer__min_df':...  \n",
       "1  {'columntransformer__countvectorizer__min_df':...  \n",
       "2  {'columntransformer__countvectorizer__min_df':...  \n",
       "3  {'columntransformer__countvectorizer__min_df':...  \n",
       "4  {'columntransformer__countvectorizer__min_df':...  "
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = pd.DataFrame(rand.cv_results_)\n",
    "results[['mean_test_score', 'std_test_score', 'params']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "a66f856d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7831247365627025\n",
      "{'columntransformer__countvectorizer__min_df': 1, 'columntransformer__countvectorizer__token_pattern': '\\\\b\\\\w\\\\w+\\\\b', 'logisticregression__C': 0.3965807272960261}\n"
     ]
    }
   ],
   "source": [
    "print(rand.best_score_)\n",
    "print(rand.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eac9542b",
   "metadata": {},
   "source": [
    "- emmmm准确率不如gridsearch中的params，所以暂且就用gridsearch中的参数作为最终结果吧"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b65e84a",
   "metadata": {},
   "source": [
    "## 4：Making new predictions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "e89eacc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new = new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "fbc84d20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('columntransformer',\n",
       "                 ColumnTransformer(transformers=[('countvectorizer',\n",
       "                                                  CountVectorizer(token_pattern='\\\\b\\\\w\\\\w+\\\\b'),\n",
       "                                                  'ingredients_str')])),\n",
       "                ('logisticregression',\n",
       "                 LogisticRegression(C=1, solver='liblinear'))])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.best_estimator_ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "b02ff8f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_pred_class_rand = grid.predict(X_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "5fb97b5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame({'id':new.id,'cuisine':new_pred_class_rand}).set_index('id').to_csv('sub_samli_01.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7c9b590",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
