# Kaggle_Contest_Samli

### My code for kaggle contest
- 01 What's cooking
  - 4/3 with CountVectorizer and LogisticRegression by tuning params with GridSearchCV
  - 4/5 using Ensembling Model & model stacking technique(failed to smoothly tune the parameters)
  - 4/7 using Ensembling Model (LogisticRegression, SVC, RandomForest) to achieve the accuracy of 0.79002
  - 4/8 using autogluon to achieve the accuracy of 0.80108

- 02 House Prices - Advanced Regression Techniques
  - 4/10 Create a Nerual Network (Linear funtion with just 1 layer or MLP, but the latter one may easily reach overfitting). Tried a bit combination of the learning_rate, weight_decay and other params. Reach a LMSE about 0.14851. （All the structure in this model are taught by Mu Li）
  - 4/15 Tried autogluon and get a LMSE of 0.12644 
