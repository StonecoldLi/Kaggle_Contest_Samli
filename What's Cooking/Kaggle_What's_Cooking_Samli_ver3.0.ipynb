{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8b94425d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\envs\\ag\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from autogluon.tabular import TabularDataset, TabularPredictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "47e90233",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "cooking = pd.read_json(\"C:/Users/lijin/Desktop/ML-text-main/data/train.json\")\n",
    "cooking.to_csv('raw_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a6ba9be4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>id</th>\n",
       "      <th>cuisine</th>\n",
       "      <th>ingredients</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>10259</td>\n",
       "      <td>greek</td>\n",
       "      <td>['romaine lettuce', 'black olives', 'grape tom...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>25693</td>\n",
       "      <td>southern_us</td>\n",
       "      <td>['plain flour', 'ground pepper', 'salt', 'toma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>20130</td>\n",
       "      <td>filipino</td>\n",
       "      <td>['eggs', 'pepper', 'salt', 'mayonaise', 'cooki...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>22213</td>\n",
       "      <td>indian</td>\n",
       "      <td>['water', 'vegetable oil', 'wheat', 'salt']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>13162</td>\n",
       "      <td>indian</td>\n",
       "      <td>['black pepper', 'shallots', 'cornflour', 'cay...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0     id      cuisine  \\\n",
       "0           0  10259        greek   \n",
       "1           1  25693  southern_us   \n",
       "2           2  20130     filipino   \n",
       "3           3  22213       indian   \n",
       "4           4  13162       indian   \n",
       "\n",
       "                                         ingredients  \n",
       "0  ['romaine lettuce', 'black olives', 'grape tom...  \n",
       "1  ['plain flour', 'ground pepper', 'salt', 'toma...  \n",
       "2  ['eggs', 'pepper', 'salt', 'mayonaise', 'cooki...  \n",
       "3        ['water', 'vegetable oil', 'wheat', 'salt']  \n",
       "4  ['black pepper', 'shallots', 'cornflour', 'cay...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = pd.read_csv(\"raw_data.csv\")\n",
    "c.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b0275b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = TabularDataset('raw_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b1ec333d",
   "metadata": {},
   "outputs": [],
   "source": [
    "id1, unname, label = 'id','Unnamed: 0','cuisine'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f77c6f38",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No path specified. Models will be saved in: \"AutogluonModels\\ag-20230408_152006\\\"\n",
      "Beginning AutoGluon training ...\n",
      "AutoGluon will save models to \"AutogluonModels\\ag-20230408_152006\\\"\n",
      "AutoGluon Version:  0.7.0\n",
      "Python Version:     3.8.16\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.22621\n",
      "Train Data Rows:    39774\n",
      "Train Data Columns: 1\n",
      "Label Column: cuisine\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'multiclass' (because dtype of label-column == object).\n",
      "\tFirst 10 (of 20) unique label values:  ['greek', 'southern_us', 'filipino', 'indian', 'jamaican', 'spanish', 'italian', 'mexican', 'chinese', 'british']\n",
      "\tIf 'multiclass' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Train Data Class Count: 20\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    9233.24 MB\n",
      "\tTrain Data (Original)  Memory Usage: 9.02 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting CategoryFeatureGenerator...\n",
      "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
      "\t\tFitting TextSpecialFeatureGenerator...\n",
      "\t\t\tFitting BinnedFeatureGenerator...\n",
      "\t\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\t\tFitting TextNgramFeatureGenerator...\n",
      "\t\t\tFitting CountVectorizer for text features: ['ingredients']\n",
      "\t\t\tCountVectorizer fit with vocabulary size = 6882\n",
      "\t\tWarning: Due to memory constraints, ngram feature count is being reduced. Allocate more memory to maximize model quality.\n",
      "\t\tReducing Vectorizer vocab size from 6882 to 6554 to avoid OOM error\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('object', ['text']) : 1 | ['ingredients']\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('category', ['text_as_category'])  :    1 | ['ingredients']\n",
      "\t\t('int', ['binned', 'text_special']) :   10 | ['ingredients.char_count', 'ingredients.word_count', 'ingredients.capital_ratio', 'ingredients.lower_ratio', 'ingredients.digit_ratio', ...]\n",
      "\t\t('int', ['text_ngram'])             : 6555 | ['__nlp__.active', '__nlp__.active dry', '__nlp__.active dry yeast', '__nlp__.added', '__nlp__.adobo', ...]\n",
      "\t29.8s = Fit runtime\n",
      "\t1 features in original data used to generate 6566 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 521.88 MB (5.7% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 33.0s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'accuracy'\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.06285513149293508, Train Rows: 37274, Val Rows: 2500\n",
      "Fitting 13 L1 models ...\n",
      "Fitting model: KNeighborsUnif ...\n",
      "\tWarning: Model is expected to require 43.65% of available memory... (20.0% is the max safe size.)\n",
      "\tNot enough memory to train KNeighborsUnif... Skipping this model.\n",
      "Fitting model: KNeighborsDist ...\n",
      "\tWarning: Model is expected to require 43.57% of available memory... (20.0% is the max safe size.)\n",
      "\tNot enough memory to train KNeighborsDist... Skipping this model.\n",
      "Fitting model: NeuralNetFastAI ...\n",
      "\tWarning: Potentially not enough memory to safely train model, roughly requires: 4.894 GB, but only 8.085 GB is available...\n",
      "\t0.2616\t = Validation score   (accuracy)\n",
      "\t92.13s\t = Training   runtime\n",
      "\t0.06s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ...\n",
      "\tWarning: Potentially not enough memory to safely train model, roughly requires: 5.872 GB, but only 8.516 GB is available...\n",
      "\t0.7948\t = Validation score   (accuracy)\n",
      "\t169.45s\t = Training   runtime\n",
      "\t1.27s\t = Validation runtime\n",
      "Fitting model: LightGBM ...\n",
      "\tWarning: Potentially not enough memory to safely train model, roughly requires: 5.872 GB, but only 8.687 GB is available...\n",
      "\t0.7892\t = Validation score   (accuracy)\n",
      "\t87.54s\t = Training   runtime\n",
      "\t0.58s\t = Validation runtime\n",
      "Fitting model: RandomForestGini ...\n",
      "\tWarning: Reducing model 'n_estimators' from 300 -> 207 due to low memory. Expected memory usage reduced from 21.7% -> 15.0% of available memory...\n",
      "\t0.74\t = Validation score   (accuracy)\n",
      "\t69.75s\t = Training   runtime\n",
      "\t0.25s\t = Validation runtime\n",
      "Fitting model: RandomForestEntr ...\n",
      "\tWarning: Reducing model 'n_estimators' from 300 -> 176 due to low memory. Expected memory usage reduced from 25.53% -> 15.0% of available memory...\n",
      "\t0.7112\t = Validation score   (accuracy)\n",
      "\t52.95s\t = Training   runtime\n",
      "\t0.32s\t = Validation runtime\n",
      "Fitting model: CatBoost ...\n",
      "\tMany features detected (6566), dynamically setting 'colsample_bylevel' to 0.15229972586049345 to speed up training (Default = 1).\n",
      "\tTo disable this functionality, explicitly specify 'colsample_bylevel' in the model hyperparameters.\n",
      "\t0.7872\t = Validation score   (accuracy)\n",
      "\t14891.66s\t = Training   runtime\n",
      "\t0.84s\t = Validation runtime\n",
      "Fitting model: ExtraTreesGini ...\n",
      "\tWarning: Reducing model 'n_estimators' from 300 -> 208 due to low memory. Expected memory usage reduced from 21.6% -> 15.0% of available memory...\n",
      "\t0.7484\t = Validation score   (accuracy)\n",
      "\t76.27s\t = Training   runtime\n",
      "\t0.46s\t = Validation runtime\n",
      "Fitting model: ExtraTreesEntr ...\n",
      "\tWarning: Reducing model 'n_estimators' from 300 -> 185 due to low memory. Expected memory usage reduced from 24.28% -> 15.0% of available memory...\n",
      "\t0.7228\t = Validation score   (accuracy)\n",
      "\t55.52s\t = Training   runtime\n",
      "\t0.35s\t = Validation runtime\n",
      "Fitting model: XGBoost ...\n",
      "\t0.7988\t = Validation score   (accuracy)\n",
      "\t478.18s\t = Training   runtime\n",
      "\t1.16s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch ...\n",
      "\t0.26\t = Validation score   (accuracy)\n",
      "\t135.03s\t = Training   runtime\n",
      "\t0.04s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ...\n",
      "\tWarning: Potentially not enough memory to safely train model, roughly requires: 5.872 GB, but only 9.469 GB is available...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's multi_error: 0.202\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t0.8\t = Validation score   (accuracy)\n",
      "\t684.05s\t = Training   runtime\n",
      "\t3.26s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ...\n",
      "\t0.8028\t = Validation score   (accuracy)\n",
      "\t1.99s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 16873.26s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230408_152006\\\")\n"
     ]
    }
   ],
   "source": [
    "predictor = TabularPredictor(label=label).fit(\n",
    "            train_data.drop(columns=[id1,unname]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3d0f2eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "cooking_test = pd.read_json(\"C:/Users/lijin/Desktop/ML-text-main/data/test.json\")\n",
    "cooking_test.to_csv('test_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "45de4857",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>id</th>\n",
       "      <th>ingredients</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>18009</td>\n",
       "      <td>['baking powder', 'eggs', 'all-purpose flour', 'raisins', 'milk', 'white sugar']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>28583</td>\n",
       "      <td>['sugar', 'egg yolks', 'corn starch', 'cream of tartar', 'bananas', 'vanilla wafers', 'milk', 'vanilla extract', 'toasted pecans', 'egg whites', 'light rum']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>41580</td>\n",
       "      <td>['sausage links', 'fennel bulb', 'fronds', 'olive oil', 'cuban peppers', 'onions']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>29752</td>\n",
       "      <td>['meat cuts', 'file powder', 'smoked sausage', 'okra', 'shrimp', 'andouille sausage', 'water', 'paprika', 'hot sauce', 'garlic cloves', 'browning', 'lump crab meat', 'vegetable oil', 'all-purpose flour', 'freshly ground pepper', 'flat leaf parsley', 'boneless chicken skinless thigh', 'dried thyme', 'white rice', 'yellow onion', 'ham']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>35687</td>\n",
       "      <td>['ground black pepper', 'salt', 'sausage casings', 'leeks', 'parmigiano reggiano cheese', 'cornmeal', 'water', 'extra-virgin olive oil']</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0     id  \\\n",
       "0           0  18009   \n",
       "1           1  28583   \n",
       "2           2  41580   \n",
       "3           3  29752   \n",
       "4           4  35687   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                        ingredients  \n",
       "0                                                                                                                                                                                                                                                                  ['baking powder', 'eggs', 'all-purpose flour', 'raisins', 'milk', 'white sugar']  \n",
       "1                                                                                                                                                                                     ['sugar', 'egg yolks', 'corn starch', 'cream of tartar', 'bananas', 'vanilla wafers', 'milk', 'vanilla extract', 'toasted pecans', 'egg whites', 'light rum']  \n",
       "2                                                                                                                                                                                                                                                                ['sausage links', 'fennel bulb', 'fronds', 'olive oil', 'cuban peppers', 'onions']  \n",
       "3  ['meat cuts', 'file powder', 'smoked sausage', 'okra', 'shrimp', 'andouille sausage', 'water', 'paprika', 'hot sauce', 'garlic cloves', 'browning', 'lump crab meat', 'vegetable oil', 'all-purpose flour', 'freshly ground pepper', 'flat leaf parsley', 'boneless chicken skinless thigh', 'dried thyme', 'white rice', 'yellow onion', 'ham']  \n",
       "4                                                                                                                                                                                                          ['ground black pepper', 'salt', 'sausage casings', 'leeks', 'parmigiano reggiano cheese', 'cornmeal', 'water', 'extra-virgin olive oil']  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = pd.read_csv(\"test_data.csv\")\n",
    "t.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4d0e9d8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loaded data from: test_data.csv | Columns = 3 / 3 | Rows = 9944 -> 9944\n"
     ]
    }
   ],
   "source": [
    "test_data = TabularDataset('test_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e2f80c1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = predictor.predict(test_data.drop(columns=[id1, unname]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "052840b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame({'id':test_data.id,'cuisine':preds}).set_index('id').to_csv('sub_samli_07.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4340d092",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ag]",
   "language": "python",
   "name": "conda-env-ag-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
